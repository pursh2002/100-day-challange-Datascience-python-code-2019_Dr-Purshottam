{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion-MNIST-GAN- Dr Pursh",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pursh2002/100-day-challange-Datascience-python-code-2019_Dr-Purshottam/blob/master/Fashion_MNIST_GAN_Dr_Pursh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB4LbeKK_gC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OwF33Mz_lD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6Dy04ZF_xvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN_WYQQC_-B_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import PIL\n",
        "print(PIL.PILLOW_VERSION)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBm_gmJQARRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHO7nJIUAUgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyAMDIClAZXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function,division\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.layers import Input, Dense , Reshape , Flatten, Dropout \n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam \n",
        "from keras.layers import BatchNormalization , Activation , ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import sys \n",
        "import numpy as np \n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qDd5qgiAd1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir MNIST_Fashion\n",
        "!cp *.gz MNIST_Fashion/\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_Fashion/\", one_hot = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8IHwVqhAshh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training PArams\n",
        "learning_rate = 0.0002\n",
        "batch_size = 128\n",
        "epochs = 100000\n",
        "\n",
        "#Network params\n",
        "image_dim = 784 #img sz is 28x28\n",
        "Y_dimension = 10 # The number of classes\n",
        "gen_hidd_dim = 256\n",
        "disc_hidd_dim  = 256\n",
        "z_noise_dim = 100\n",
        "\n",
        "def xavier_init(shape):\n",
        "  return tf.random_normal(shape = shape, stddev= 1./tf.sqrt(shape[0]/2.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umH66z4BAw84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = {\n",
        "    \"disc_H\" : tf.Variable(xavier_init([image_dim + Y_dimension, disc_hidd_dim])),\n",
        "    \"disc_final\": tf.Variable(xavier_init([disc_hidd_dim,1])),\n",
        "    \"gen_H\": tf.Variable(xavier_init([z_noise_dim + Y_dimension, gen_hidd_dim])),\n",
        "    \"gen_final\": tf.Variable(xavier_init([gen_hidd_dim, image_dim]))\n",
        "}\n",
        "\n",
        "bias = {\n",
        "    \"disc_H\" : tf.Variable(xavier_init([disc_hidd_dim])),\n",
        "    \"disc_final\": tf.Variable(xavier_init([1])),\n",
        "    \"gen_H\": tf.Variable(xavier_init([gen_hidd_dim])),\n",
        "    \"gen_final\": tf.Variable(xavier_init([image_dim]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8Ej5vMKA0C3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define placeholders for external input\n",
        "\n",
        "z_input = tf.placeholder(tf.float32, shape = [None, z_noise_dim], name = \"input_noise\")\n",
        "x_input = tf.placeholder(tf.float32, shape = [None, image_dim], name = \"real_input\")\n",
        "Y_input = tf.placeholder(tf.float32, shape = [None, Y_dimension], name = \"Labels\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7OeoQ4yA3MG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Discriminator(x,y):\n",
        "  inputs = tf.concat(axis = 1, values = [x,y])\n",
        "  hidden_layer = tf.nn.relu(tf.add(tf.matmul(inputs, weights[\"disc_H\"]), bias[\"disc_H\"]))\n",
        "  final_layer = (tf.add(tf.matmul(hidden_layer, weights[\"disc_final\"]), bias[\"disc_final\"]))\n",
        "  disc_output = tf.nn.sigmoid(final_layer)\n",
        "  return final_layer, disc_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLahcaZ4A_bW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generator NW\n",
        "def Generator(x,y):\n",
        "  inputs = tf.concat(axis = 1, values = [x,y])\n",
        "  hidden_layer = tf.nn.relu(tf.add(tf.matmul(inputs, weights[\"gen_H\"]), bias[\"gen_H\"]))\n",
        "  final_layer = (tf.add(tf.matmul(hidden_layer, weights[\"gen_final\"]), bias[\"gen_final\"]))\n",
        "  gen_output = tf.nn.sigmoid(final_layer)\n",
        "  return gen_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFcVezPOBCGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building the GEN NW\n",
        "output_Gen = Generator(z_input, Y_input) #G(z/y)\n",
        " \n",
        " # Building the Disc NW\n",
        "real_output1_Disc, real_output_disc = Discriminator(x_input, Y_input) #implements D(x/y)\n",
        "fake_output1_Disc, fake_output_disc = Discriminator(output_Gen, Y_input) # implements D(G(x/y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ia6whKBBHo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#first kind of loss\n",
        "with tf.name_scope(\"Discriminator_Loss\") as scope:\n",
        "  Discriminator_Loss = -tf.reduce_mean(tf.log(real_output_disc+ 0.0001)+tf.log(1.- fake_output_disc+0.0001))\n",
        "  \n",
        "with tf.name_scope(\"Genetator_Loss\") as scope:\n",
        "  Generator_Loss = -tf.reduce_mean(tf.log(fake_output_disc+ 0.0001)) # due to max log(D(G(x)))\n",
        "\n",
        "  # T-board summary\n",
        "  \n",
        "  Disc_loss_total = tf.summary.scalar(\"Disc_Total_loss\", Discriminator_Loss)\n",
        "  Gen_loss_total = tf.summary.scalar(\"Gen_loss\", Generator_Loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuDRJrq0BPnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the variables\n",
        "\n",
        "Generator_var = [weights[\"gen_H\"], weights[\"gen_final\"], bias[\"gen_H\"], bias[\"gen_final\"]]\n",
        "Discriminator_var = [weights[\"disc_H\"], weights[\"disc_final\"], bias[\"disc_H\"], bias[\"disc_final\"]]\n",
        "\n",
        "#Define the optimizer\n",
        "with tf.name_scope(\"Optimizer_Discriminator\") as scope:\n",
        "  Discriminator_optimize = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(Discriminator_Loss, var_list = Discriminator_var)\n",
        "\n",
        "with tf.name_scope(\"Optimizer_Generator\") as scope:\n",
        "  Generator_optimize = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(Generator_Loss, var_list = Generator_var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywUH9GxDBWsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the variables\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "\n",
        "sess.run(init)\n",
        "writer = tf.summary.FileWriter(\"./log\", sess.graph)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  x_batch, Y_label = mnist.train.next_batch(batch_size)\n",
        "  \n",
        "  #Generate noise to feed Discriminator\n",
        "  z_noise = np.random.uniform(-1.,1.,size = [batch_size, z_noise_dim])\n",
        "  _, Disc_loss_epoch = sess.run([Discriminator_optimize, Discriminator_Loss], feed_dict = {x_input:x_batch, Y_input:Y_label, z_input:z_noise})\n",
        "  _, Gen_loss_epoch = sess.run([Generator_optimize, Generator_Loss], feed_dict = {z_input:z_noise, Y_input:Y_label})  \n",
        "  \n",
        "  #Running the Discriminator summary\n",
        "  summary_Disc_loss = sess.run(Disc_loss_total, feed_dict = {x_input:x_batch, z_input:z_noise, Y_input:Y_label})\n",
        "  # Adding the Discriminator summary\n",
        "  writer.add_summary(summary_Disc_loss, epoch)\n",
        "  \n",
        "  #Running the Generator summary\n",
        "  summary_Gen_loss = sess.run(Gen_loss_total, feed_dict = {z_input:z_noise, Y_input:Y_label})\n",
        "  # Adding the Generator summary\n",
        "  writer.add_summary(summary_Gen_loss, epoch)\n",
        "  \n",
        "  if epoch % 2000 == 0:\n",
        "    print(\"Steps: {0}: Generator Loss: {1}, Discriminator Loss:{2}\".format(epoch, Gen_loss_epoch, Disc_loss_epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXdLOy6rBa0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x_batch.shape)\n",
        "print(x_input.shape)\n",
        "print(Y_input.shape)\n",
        "print(Y_label.shape)\n",
        "print(z_input.shape)\n",
        "print(z_noise.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zn35-0qFoJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_plot(samples):\n",
        "  fig = plt.figure(figsize = (4,4))\n",
        "  gs = gridspec.GridSpec(4,4)\n",
        "  gs.update(wspace = 0.05, hspace = 0.05)\n",
        "  \n",
        "  for i, sample in enumerate(samples):\n",
        "    ax = plt.subplot(gs[i])\n",
        "    plt.axis('off')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_aspect('equal')\n",
        "    plt.imshow(sample.reshape(28,28), cmap = 'gray')\n",
        "  return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrrG5iOrFx1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create(inp):\n",
        "  feature_map = { \"t-shirt\":0,\n",
        "                 \"trouser\":1,\n",
        "                 \"pullover\":2,\n",
        "                 \"dress\":3,\n",
        "                 \"coat\":4,\n",
        "                 \"sandal\":5,\n",
        "                 \"sirt\":6,\n",
        "                 \"sneaker\":7,\n",
        "                 \"bag\":8,\n",
        "                 \"ankle boot\": 9\n",
        "                }\n",
        "  samples = 16\n",
        "  z_noise = np.random.uniform(-1.,1.,size = [samples, z_noise_dim])\n",
        "  #one hot encoding\n",
        "  Y_label = np.zeros(shape = [samples, Y_dimension])\n",
        "  Y_label[:, feature_map[inp]] = 1\n",
        "  \n",
        "  # run the traineg generator excluding Discriminator\n",
        "  generated_samples = sess.run(output_Gen, feed_dict = {z_input:z_noise, Y_input:Y_label})\n",
        "  #plot images\n",
        "  \n",
        "  generate_plot(generated_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMairkaNF1fy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create(\"coat\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsRdIPvxF4Ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create(\"sandal\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxg_Ls0cGBB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create(\"sneaker\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC8Z0RdYGJeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create('trouser')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqMOsAOfGQdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create('t-shirt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccUR3LQHGWud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}